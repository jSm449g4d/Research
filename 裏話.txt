Q.なんで超解像?
去年の研究テーマは「文章要約のフレームワークを提唱する」だったんだけど、「文章要約の評価方法」をろくに考えずに進めた結果、行き詰まってテーマを変える必要に迫られた。
次のテーマは「背景」「課題」「評価」をしっかり固めてから研究に取り組む必要がある。
そんな時に時に教授から
「今ではビッグデータ前提で性能だけ求める研究多いけど、限られたデータから情報処理しなければいけないケースってあるよね。"エビデンス的に"」
って助言を頂いた。
実は元々は生成系AI作りたかった関係で超解像(SR)や画像補完(ImageInpainting)の論文も読み漁ってた。その時を反芻したところ、最近GAN使う事前提で、つまりVGG19とか流用する研究ばっかりだなって思った。
去年同期も、今の私と同じような事やってて「VDSRうまくいかねぇ」って悩んでたので、「限られたデータで超解像とかできるモデルがあれば有用性あるんじゃない？」と思ったので話を進めた。
背景: 限られたデータだけで処理しないといけない(ビッグデータ使えない)盤面あるよね
課題: 「出来るだけ少ないデータ数」で「データを高品質」にする
評価: PSNR,SSIMでバッチリ数値化
手法: SRCNNを軸に現代技術で高性能にしていく
という研究指針を立てて開始した。

最初の壁
正直、最初はSRCNNをベースにして改良していけばよいと思ってた...が現実は甘くなかった。
ベースラインのSRCNNがキュービック法と呼ばれる、「単純に画像を引き延ばす」に毛が生えた手法に勝てなかった。
元の論文とかだと、今みたいな「データ数制限」とかないのと、私の実装が悪かった可能性もある...
というわけで片っ端から思いついたり見かけた機械学習モデルを試すことにした。
ネタばれになるが現在(2020/09/01)の所、U-NETの簡略版がほぼ唯一キュービック変換に勝つ可能性があるという状態である。
恐らく、低解像度領域と元解像度領域の二つを残差接続させることにより、両者の特性を組み合わせた認識が可能であるのではないかと考えている。
なお、オリジナルのU-NETに近い構成にすると「問題に対して層の数が多すぎる」ためか精度が悪化するだけとなった。

タスクにノイズ除去を加えている理由
実は保険である。U-NETの簡略版が見つかるまでは、キュービック法に勝てないという可能性もあったからだ。
背景的には「データの高品質化」であるので、ノイズ除去でも文脈は成り立つのだ。
加えるなら、「データの高品質化」が背景なのに、「超解像以外やってません」は流石に厳しい。

